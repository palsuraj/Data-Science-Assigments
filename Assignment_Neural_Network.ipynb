{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_Neural_Network.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YG-0/Datascience_assisgnments/blob/main/Assignment_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSV1PfDiIM_M"
      },
      "source": [
        "# **Forestfires**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nssQk6ByDu0Y",
        "outputId": "df030c81-c71d-4ff7-c4d5-27b25f561dbb"
      },
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (5.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.41.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xae4rX2YDuyu"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfqhqqUVDuwY"
      },
      "source": [
        "df=pd.read_csv('forestfires.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "d-8omqnWDuvx",
        "outputId": "f2331e59-5a07-476a-a271-703584155462"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>FFMC</th>\n",
              "      <th>DMC</th>\n",
              "      <th>DC</th>\n",
              "      <th>ISI</th>\n",
              "      <th>temp</th>\n",
              "      <th>RH</th>\n",
              "      <th>wind</th>\n",
              "      <th>rain</th>\n",
              "      <th>area</th>\n",
              "      <th>dayfri</th>\n",
              "      <th>daymon</th>\n",
              "      <th>daysat</th>\n",
              "      <th>daysun</th>\n",
              "      <th>daythu</th>\n",
              "      <th>daytue</th>\n",
              "      <th>daywed</th>\n",
              "      <th>monthapr</th>\n",
              "      <th>monthaug</th>\n",
              "      <th>monthdec</th>\n",
              "      <th>monthfeb</th>\n",
              "      <th>monthjan</th>\n",
              "      <th>monthjul</th>\n",
              "      <th>monthjun</th>\n",
              "      <th>monthmar</th>\n",
              "      <th>monthmay</th>\n",
              "      <th>monthnov</th>\n",
              "      <th>monthoct</th>\n",
              "      <th>monthsep</th>\n",
              "      <th>size_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mar</td>\n",
              "      <td>fri</td>\n",
              "      <td>86.2</td>\n",
              "      <td>26.2</td>\n",
              "      <td>94.3</td>\n",
              "      <td>5.1</td>\n",
              "      <td>8.2</td>\n",
              "      <td>51</td>\n",
              "      <td>6.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>oct</td>\n",
              "      <td>tue</td>\n",
              "      <td>90.6</td>\n",
              "      <td>35.4</td>\n",
              "      <td>669.1</td>\n",
              "      <td>6.7</td>\n",
              "      <td>18.0</td>\n",
              "      <td>33</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>oct</td>\n",
              "      <td>sat</td>\n",
              "      <td>90.6</td>\n",
              "      <td>43.7</td>\n",
              "      <td>686.9</td>\n",
              "      <td>6.7</td>\n",
              "      <td>14.6</td>\n",
              "      <td>33</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mar</td>\n",
              "      <td>fri</td>\n",
              "      <td>91.7</td>\n",
              "      <td>33.3</td>\n",
              "      <td>77.5</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.3</td>\n",
              "      <td>97</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mar</td>\n",
              "      <td>sun</td>\n",
              "      <td>89.3</td>\n",
              "      <td>51.3</td>\n",
              "      <td>102.2</td>\n",
              "      <td>9.6</td>\n",
              "      <td>11.4</td>\n",
              "      <td>99</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>aug</td>\n",
              "      <td>sun</td>\n",
              "      <td>81.6</td>\n",
              "      <td>56.7</td>\n",
              "      <td>665.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>27.8</td>\n",
              "      <td>32</td>\n",
              "      <td>2.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.44</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>aug</td>\n",
              "      <td>sun</td>\n",
              "      <td>81.6</td>\n",
              "      <td>56.7</td>\n",
              "      <td>665.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>21.9</td>\n",
              "      <td>71</td>\n",
              "      <td>5.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>aug</td>\n",
              "      <td>sun</td>\n",
              "      <td>81.6</td>\n",
              "      <td>56.7</td>\n",
              "      <td>665.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>21.2</td>\n",
              "      <td>70</td>\n",
              "      <td>6.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>aug</td>\n",
              "      <td>sat</td>\n",
              "      <td>94.4</td>\n",
              "      <td>146.0</td>\n",
              "      <td>614.7</td>\n",
              "      <td>11.3</td>\n",
              "      <td>25.6</td>\n",
              "      <td>42</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>nov</td>\n",
              "      <td>tue</td>\n",
              "      <td>79.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>106.7</td>\n",
              "      <td>1.1</td>\n",
              "      <td>11.8</td>\n",
              "      <td>31</td>\n",
              "      <td>4.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>517 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    month  day  FFMC    DMC  ...  monthnov  monthoct  monthsep  size_category\n",
              "0     mar  fri  86.2   26.2  ...         0         0         0          small\n",
              "1     oct  tue  90.6   35.4  ...         0         1         0          small\n",
              "2     oct  sat  90.6   43.7  ...         0         1         0          small\n",
              "3     mar  fri  91.7   33.3  ...         0         0         0          small\n",
              "4     mar  sun  89.3   51.3  ...         0         0         0          small\n",
              "..    ...  ...   ...    ...  ...       ...       ...       ...            ...\n",
              "512   aug  sun  81.6   56.7  ...         0         0         0          large\n",
              "513   aug  sun  81.6   56.7  ...         0         0         0          large\n",
              "514   aug  sun  81.6   56.7  ...         0         0         0          large\n",
              "515   aug  sat  94.4  146.0  ...         0         0         0          small\n",
              "516   nov  tue  79.5    3.0  ...         1         0         0          small\n",
              "\n",
              "[517 rows x 31 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B_ARfnCDus-"
      },
      "source": [
        "df1=df.drop(['month','day'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qVinUPoDupl"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_EJ0fv_Duoo"
      },
      "source": [
        "lb=LabelEncoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aABhtXeEDulw"
      },
      "source": [
        "df1['size_category']=lb.fit_transform(df1['size_category'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwL6lbCyDulB",
        "outputId": "5821ab75-879d-44fd-92dd-7ed2b4891bcc"
      },
      "source": [
        "df2=df1.values\n",
        "df2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(517, 29)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cl-Q8qKjDuhX",
        "outputId": "e7773e24-5658-4d7a-db80-a1bf427454a7"
      },
      "source": [
        "x=df2[:,0:28]\n",
        "y=df2[:,-1]\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(517, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeCqL2M6DueY"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBMi9gweDudu"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=28, activation='relu'))\n",
        "model.add(Dense(28, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIGNb4VoDuah"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYRA18P0DuZu",
        "outputId": "fbff9cbe-df02-42ac-c5a9-5f43c9cd93f8"
      },
      "source": [
        "model.fit(x, y, validation_split=0.33,epochs=100, batch_size=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "70/70 [==============================] - 1s 5ms/step - loss: 0.8964 - accuracy: 0.7775 - val_loss: 0.8329 - val_accuracy: 0.8070\n",
            "Epoch 2/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3991 - accuracy: 0.8555 - val_loss: 0.6320 - val_accuracy: 0.8129\n",
            "Epoch 3/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3059 - accuracy: 0.8931 - val_loss: 0.4906 - val_accuracy: 0.8304\n",
            "Epoch 4/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.2303 - accuracy: 0.9046 - val_loss: 0.3733 - val_accuracy: 0.8421\n",
            "Epoch 5/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.2003 - accuracy: 0.9220 - val_loss: 0.3336 - val_accuracy: 0.8596\n",
            "Epoch 6/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.2114 - accuracy: 0.9220 - val_loss: 0.3546 - val_accuracy: 0.8304\n",
            "Epoch 7/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.3331 - accuracy: 0.8584 - val_loss: 0.2625 - val_accuracy: 0.9006\n",
            "Epoch 8/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.1307 - accuracy: 0.9538 - val_loss: 0.2239 - val_accuracy: 0.9123\n",
            "Epoch 9/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1087 - accuracy: 0.9653 - val_loss: 0.2723 - val_accuracy: 0.9181\n",
            "Epoch 10/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0963 - accuracy: 0.9740 - val_loss: 0.3607 - val_accuracy: 0.8889\n",
            "Epoch 11/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1363 - accuracy: 0.9422 - val_loss: 0.2211 - val_accuracy: 0.9123\n",
            "Epoch 12/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1066 - accuracy: 0.9653 - val_loss: 0.3441 - val_accuracy: 0.8947\n",
            "Epoch 13/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0850 - accuracy: 0.9711 - val_loss: 0.5593 - val_accuracy: 0.8655\n",
            "Epoch 14/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1156 - accuracy: 0.9566 - val_loss: 0.3224 - val_accuracy: 0.8596\n",
            "Epoch 15/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0925 - accuracy: 0.9653 - val_loss: 0.2407 - val_accuracy: 0.9123\n",
            "Epoch 16/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0681 - accuracy: 0.9769 - val_loss: 0.2834 - val_accuracy: 0.9240\n",
            "Epoch 17/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0615 - accuracy: 0.9855 - val_loss: 0.4077 - val_accuracy: 0.8830\n",
            "Epoch 18/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1659 - accuracy: 0.9509 - val_loss: 0.1957 - val_accuracy: 0.9240\n",
            "Epoch 19/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0907 - accuracy: 0.9624 - val_loss: 0.2332 - val_accuracy: 0.9064\n",
            "Epoch 20/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0624 - accuracy: 0.9711 - val_loss: 0.2218 - val_accuracy: 0.9064\n",
            "Epoch 21/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0723 - accuracy: 0.9740 - val_loss: 0.2029 - val_accuracy: 0.9298\n",
            "Epoch 22/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.0900 - accuracy: 0.9769 - val_loss: 0.2552 - val_accuracy: 0.9181\n",
            "Epoch 23/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0543 - accuracy: 0.9798 - val_loss: 0.1787 - val_accuracy: 0.9357\n",
            "Epoch 24/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.2082 - accuracy: 0.9393 - val_loss: 0.1725 - val_accuracy: 0.9357\n",
            "Epoch 25/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0722 - accuracy: 0.9682 - val_loss: 0.3759 - val_accuracy: 0.9064\n",
            "Epoch 26/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0659 - accuracy: 0.9798 - val_loss: 0.1678 - val_accuracy: 0.9240\n",
            "Epoch 27/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0520 - accuracy: 0.9798 - val_loss: 0.2600 - val_accuracy: 0.9123\n",
            "Epoch 28/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0564 - accuracy: 0.9798 - val_loss: 0.1657 - val_accuracy: 0.9357\n",
            "Epoch 29/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0509 - accuracy: 0.9884 - val_loss: 0.2497 - val_accuracy: 0.9181\n",
            "Epoch 30/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0524 - accuracy: 0.9855 - val_loss: 0.2104 - val_accuracy: 0.9181\n",
            "Epoch 31/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0474 - accuracy: 0.9855 - val_loss: 0.1852 - val_accuracy: 0.9357\n",
            "Epoch 32/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0558 - accuracy: 0.9827 - val_loss: 0.2212 - val_accuracy: 0.9181\n",
            "Epoch 33/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0295 - accuracy: 0.9913 - val_loss: 0.1983 - val_accuracy: 0.9240\n",
            "Epoch 34/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9855 - val_loss: 0.3546 - val_accuracy: 0.9181\n",
            "Epoch 35/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9827 - val_loss: 0.3640 - val_accuracy: 0.9181\n",
            "Epoch 36/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0837 - accuracy: 0.9711 - val_loss: 0.1696 - val_accuracy: 0.9415\n",
            "Epoch 37/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0756 - accuracy: 0.9711 - val_loss: 0.2752 - val_accuracy: 0.9181\n",
            "Epoch 38/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0385 - accuracy: 0.9827 - val_loss: 0.2450 - val_accuracy: 0.9240\n",
            "Epoch 39/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0477 - accuracy: 0.9798 - val_loss: 0.5061 - val_accuracy: 0.9006\n",
            "Epoch 40/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0919 - accuracy: 0.9595 - val_loss: 0.2150 - val_accuracy: 0.9181\n",
            "Epoch 41/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.0387 - accuracy: 0.9855 - val_loss: 0.2117 - val_accuracy: 0.9298\n",
            "Epoch 42/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0535 - accuracy: 0.9798 - val_loss: 0.3213 - val_accuracy: 0.9123\n",
            "Epoch 43/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0458 - accuracy: 0.9798 - val_loss: 0.2448 - val_accuracy: 0.9240\n",
            "Epoch 44/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1406 - accuracy: 0.9538 - val_loss: 0.3139 - val_accuracy: 0.9123\n",
            "Epoch 45/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0885 - accuracy: 0.9682 - val_loss: 0.4552 - val_accuracy: 0.9064\n",
            "Epoch 46/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0590 - accuracy: 0.9798 - val_loss: 0.1421 - val_accuracy: 0.9474\n",
            "Epoch 47/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9769 - val_loss: 0.2234 - val_accuracy: 0.9357\n",
            "Epoch 48/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0348 - accuracy: 0.9913 - val_loss: 0.3235 - val_accuracy: 0.9298\n",
            "Epoch 49/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0543 - accuracy: 0.9827 - val_loss: 0.6715 - val_accuracy: 0.8830\n",
            "Epoch 50/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0569 - accuracy: 0.9769 - val_loss: 0.1847 - val_accuracy: 0.9415\n",
            "Epoch 51/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1052 - accuracy: 0.9711 - val_loss: 0.2449 - val_accuracy: 0.9298\n",
            "Epoch 52/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.0315 - accuracy: 0.9855 - val_loss: 0.5108 - val_accuracy: 0.9064\n",
            "Epoch 53/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0659 - accuracy: 0.9769 - val_loss: 0.1639 - val_accuracy: 0.9415\n",
            "Epoch 54/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0592 - accuracy: 0.9769 - val_loss: 0.1543 - val_accuracy: 0.9415\n",
            "Epoch 55/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.0345 - accuracy: 0.9855 - val_loss: 0.4107 - val_accuracy: 0.9123\n",
            "Epoch 56/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0721 - accuracy: 0.9769 - val_loss: 0.2132 - val_accuracy: 0.9357\n",
            "Epoch 57/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9855 - val_loss: 0.1992 - val_accuracy: 0.9357\n",
            "Epoch 58/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0745 - accuracy: 0.9769 - val_loss: 0.4008 - val_accuracy: 0.9181\n",
            "Epoch 59/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9798 - val_loss: 0.3257 - val_accuracy: 0.9240\n",
            "Epoch 60/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.9884 - val_loss: 0.1982 - val_accuracy: 0.9357\n",
            "Epoch 61/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 0.9942 - val_loss: 0.1707 - val_accuracy: 0.9415\n",
            "Epoch 62/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 0.9913 - val_loss: 0.2481 - val_accuracy: 0.9298\n",
            "Epoch 63/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0389 - accuracy: 0.9827 - val_loss: 0.6204 - val_accuracy: 0.9006\n",
            "Epoch 64/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0306 - accuracy: 0.9913 - val_loss: 0.2345 - val_accuracy: 0.9298\n",
            "Epoch 65/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0216 - accuracy: 0.9913 - val_loss: 0.4105 - val_accuracy: 0.9181\n",
            "Epoch 66/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.9798 - val_loss: 0.1791 - val_accuracy: 0.9415\n",
            "Epoch 67/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1087 - accuracy: 0.9653 - val_loss: 0.2127 - val_accuracy: 0.9240\n",
            "Epoch 68/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0447 - accuracy: 0.9827 - val_loss: 0.2317 - val_accuracy: 0.9357\n",
            "Epoch 69/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 0.9942 - val_loss: 0.3993 - val_accuracy: 0.9123\n",
            "Epoch 70/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1201 - accuracy: 0.9653 - val_loss: 0.8131 - val_accuracy: 0.8655\n",
            "Epoch 71/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0914 - accuracy: 0.9769 - val_loss: 0.1664 - val_accuracy: 0.9298\n",
            "Epoch 72/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0184 - accuracy: 0.9971 - val_loss: 0.2459 - val_accuracy: 0.9357\n",
            "Epoch 73/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0293 - accuracy: 0.9884 - val_loss: 0.2335 - val_accuracy: 0.9298\n",
            "Epoch 74/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0654 - accuracy: 0.9769 - val_loss: 0.1341 - val_accuracy: 0.9474\n",
            "Epoch 75/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0196 - accuracy: 0.9942 - val_loss: 0.4331 - val_accuracy: 0.9123\n",
            "Epoch 76/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9798 - val_loss: 0.4538 - val_accuracy: 0.9123\n",
            "Epoch 77/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9769 - val_loss: 0.2077 - val_accuracy: 0.9240\n",
            "Epoch 78/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0293 - accuracy: 0.9855 - val_loss: 0.3773 - val_accuracy: 0.9240\n",
            "Epoch 79/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0450 - accuracy: 0.9827 - val_loss: 0.2973 - val_accuracy: 0.9298\n",
            "Epoch 80/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0660 - accuracy: 0.9827 - val_loss: 0.3378 - val_accuracy: 0.9064\n",
            "Epoch 81/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.9913 - val_loss: 0.2335 - val_accuracy: 0.9298\n",
            "Epoch 82/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0418 - accuracy: 0.9855 - val_loss: 0.1685 - val_accuracy: 0.9298\n",
            "Epoch 83/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.9971 - val_loss: 0.1898 - val_accuracy: 0.9298\n",
            "Epoch 84/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.0334 - accuracy: 0.9884 - val_loss: 0.2239 - val_accuracy: 0.9357\n",
            "Epoch 85/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0871 - accuracy: 0.9682 - val_loss: 0.4454 - val_accuracy: 0.9006\n",
            "Epoch 86/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0294 - accuracy: 0.9884 - val_loss: 0.1493 - val_accuracy: 0.9532\n",
            "Epoch 87/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.9913 - val_loss: 0.5900 - val_accuracy: 0.9064\n",
            "Epoch 88/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0631 - accuracy: 0.9798 - val_loss: 0.2946 - val_accuracy: 0.9298\n",
            "Epoch 89/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0168 - accuracy: 0.9942 - val_loss: 0.2164 - val_accuracy: 0.9298\n",
            "Epoch 90/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0733 - accuracy: 0.9740 - val_loss: 0.3465 - val_accuracy: 0.9357\n",
            "Epoch 91/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0756 - accuracy: 0.9769 - val_loss: 0.2897 - val_accuracy: 0.9123\n",
            "Epoch 92/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0288 - accuracy: 0.9913 - val_loss: 0.2536 - val_accuracy: 0.9298\n",
            "Epoch 93/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0590 - accuracy: 0.9769 - val_loss: 0.2731 - val_accuracy: 0.9415\n",
            "Epoch 94/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.0367 - accuracy: 0.9913 - val_loss: 0.2894 - val_accuracy: 0.9240\n",
            "Epoch 95/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.0387 - accuracy: 0.9827 - val_loss: 0.8545 - val_accuracy: 0.8713\n",
            "Epoch 96/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1360 - accuracy: 0.9624 - val_loss: 0.1750 - val_accuracy: 0.9415\n",
            "Epoch 97/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0369 - accuracy: 0.9827 - val_loss: 0.2727 - val_accuracy: 0.9415\n",
            "Epoch 98/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0295 - accuracy: 0.9884 - val_loss: 0.3345 - val_accuracy: 0.9298\n",
            "Epoch 99/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0538 - accuracy: 0.9827 - val_loss: 0.2400 - val_accuracy: 0.9357\n",
            "Epoch 100/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 0.9942 - val_loss: 0.2299 - val_accuracy: 0.9298\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f247f7fced0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obIrKhbzEqgG",
        "outputId": "d4ebf9c7-138a-4fdb-b00a-33f8c478b3a4"
      },
      "source": [
        "scores=model.evaluate(x,y)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.9749\n",
            "accuracy: 97.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR7V66vmFL19"
      },
      "source": [
        "# **Gas Turbine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1D_cYc4EqaW"
      },
      "source": [
        "df=pd.read_csv(\"gas_turbines.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "u5LzyTWkEqZd",
        "outputId": "d3f08066-7c1e-4152-b57f-34adbb5a5ae0"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AT</th>\n",
              "      <th>AP</th>\n",
              "      <th>AH</th>\n",
              "      <th>AFDP</th>\n",
              "      <th>GTEP</th>\n",
              "      <th>TIT</th>\n",
              "      <th>TAT</th>\n",
              "      <th>TEY</th>\n",
              "      <th>CDP</th>\n",
              "      <th>CO</th>\n",
              "      <th>NOX</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.8594</td>\n",
              "      <td>1007.9</td>\n",
              "      <td>96.799</td>\n",
              "      <td>3.5000</td>\n",
              "      <td>19.663</td>\n",
              "      <td>1059.2</td>\n",
              "      <td>550.00</td>\n",
              "      <td>114.70</td>\n",
              "      <td>10.605</td>\n",
              "      <td>3.1547</td>\n",
              "      <td>82.722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.7850</td>\n",
              "      <td>1008.4</td>\n",
              "      <td>97.118</td>\n",
              "      <td>3.4998</td>\n",
              "      <td>19.728</td>\n",
              "      <td>1059.3</td>\n",
              "      <td>550.00</td>\n",
              "      <td>114.72</td>\n",
              "      <td>10.598</td>\n",
              "      <td>3.2363</td>\n",
              "      <td>82.776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.8977</td>\n",
              "      <td>1008.8</td>\n",
              "      <td>95.939</td>\n",
              "      <td>3.4824</td>\n",
              "      <td>19.779</td>\n",
              "      <td>1059.4</td>\n",
              "      <td>549.87</td>\n",
              "      <td>114.71</td>\n",
              "      <td>10.601</td>\n",
              "      <td>3.2012</td>\n",
              "      <td>82.468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.0569</td>\n",
              "      <td>1009.2</td>\n",
              "      <td>95.249</td>\n",
              "      <td>3.4805</td>\n",
              "      <td>19.792</td>\n",
              "      <td>1059.6</td>\n",
              "      <td>549.99</td>\n",
              "      <td>114.72</td>\n",
              "      <td>10.606</td>\n",
              "      <td>3.1923</td>\n",
              "      <td>82.670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.3978</td>\n",
              "      <td>1009.7</td>\n",
              "      <td>95.150</td>\n",
              "      <td>3.4976</td>\n",
              "      <td>19.765</td>\n",
              "      <td>1059.7</td>\n",
              "      <td>549.98</td>\n",
              "      <td>114.72</td>\n",
              "      <td>10.612</td>\n",
              "      <td>3.2484</td>\n",
              "      <td>82.311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15034</th>\n",
              "      <td>9.0301</td>\n",
              "      <td>1005.6</td>\n",
              "      <td>98.460</td>\n",
              "      <td>3.5421</td>\n",
              "      <td>19.164</td>\n",
              "      <td>1049.7</td>\n",
              "      <td>546.21</td>\n",
              "      <td>111.61</td>\n",
              "      <td>10.400</td>\n",
              "      <td>4.5186</td>\n",
              "      <td>79.559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15035</th>\n",
              "      <td>7.8879</td>\n",
              "      <td>1005.9</td>\n",
              "      <td>99.093</td>\n",
              "      <td>3.5059</td>\n",
              "      <td>19.414</td>\n",
              "      <td>1046.3</td>\n",
              "      <td>543.22</td>\n",
              "      <td>111.78</td>\n",
              "      <td>10.433</td>\n",
              "      <td>4.8470</td>\n",
              "      <td>79.917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15036</th>\n",
              "      <td>7.2647</td>\n",
              "      <td>1006.3</td>\n",
              "      <td>99.496</td>\n",
              "      <td>3.4770</td>\n",
              "      <td>19.530</td>\n",
              "      <td>1037.7</td>\n",
              "      <td>537.32</td>\n",
              "      <td>110.19</td>\n",
              "      <td>10.483</td>\n",
              "      <td>7.9632</td>\n",
              "      <td>90.912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15037</th>\n",
              "      <td>7.0060</td>\n",
              "      <td>1006.8</td>\n",
              "      <td>99.008</td>\n",
              "      <td>3.4486</td>\n",
              "      <td>19.377</td>\n",
              "      <td>1043.2</td>\n",
              "      <td>541.24</td>\n",
              "      <td>110.74</td>\n",
              "      <td>10.533</td>\n",
              "      <td>6.2494</td>\n",
              "      <td>93.227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15038</th>\n",
              "      <td>6.9279</td>\n",
              "      <td>1007.2</td>\n",
              "      <td>97.533</td>\n",
              "      <td>3.4275</td>\n",
              "      <td>19.306</td>\n",
              "      <td>1049.9</td>\n",
              "      <td>545.85</td>\n",
              "      <td>111.58</td>\n",
              "      <td>10.583</td>\n",
              "      <td>4.9816</td>\n",
              "      <td>92.498</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15039 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           AT      AP      AH    AFDP  ...     TEY     CDP      CO     NOX\n",
              "0      6.8594  1007.9  96.799  3.5000  ...  114.70  10.605  3.1547  82.722\n",
              "1      6.7850  1008.4  97.118  3.4998  ...  114.72  10.598  3.2363  82.776\n",
              "2      6.8977  1008.8  95.939  3.4824  ...  114.71  10.601  3.2012  82.468\n",
              "3      7.0569  1009.2  95.249  3.4805  ...  114.72  10.606  3.1923  82.670\n",
              "4      7.3978  1009.7  95.150  3.4976  ...  114.72  10.612  3.2484  82.311\n",
              "...       ...     ...     ...     ...  ...     ...     ...     ...     ...\n",
              "15034  9.0301  1005.6  98.460  3.5421  ...  111.61  10.400  4.5186  79.559\n",
              "15035  7.8879  1005.9  99.093  3.5059  ...  111.78  10.433  4.8470  79.917\n",
              "15036  7.2647  1006.3  99.496  3.4770  ...  110.19  10.483  7.9632  90.912\n",
              "15037  7.0060  1006.8  99.008  3.4486  ...  110.74  10.533  6.2494  93.227\n",
              "15038  6.9279  1007.2  97.533  3.4275  ...  111.58  10.583  4.9816  92.498\n",
              "\n",
              "[15039 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEzjhAMQFkLL",
        "outputId": "308c9c88-1d9a-46b8-becb-225346eddf7f"
      },
      "source": [
        "df1=df.values\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   6.8594, 1007.9   ,   96.799 , ...,   10.605 ,    3.1547,\n",
              "          82.722 ],\n",
              "       [   6.785 , 1008.4   ,   97.118 , ...,   10.598 ,    3.2363,\n",
              "          82.776 ],\n",
              "       [   6.8977, 1008.8   ,   95.939 , ...,   10.601 ,    3.2012,\n",
              "          82.468 ],\n",
              "       ...,\n",
              "       [   7.2647, 1006.3   ,   99.496 , ...,   10.483 ,    7.9632,\n",
              "          90.912 ],\n",
              "       [   7.006 , 1006.8   ,   99.008 , ...,   10.533 ,    6.2494,\n",
              "          93.227 ],\n",
              "       [   6.9279, 1007.2   ,   97.533 , ...,   10.583 ,    4.9816,\n",
              "          92.498 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkJGVgQhFkKe",
        "outputId": "95ad6017-83b5-4f05-8db4-e243648d48a1"
      },
      "source": [
        "X=df1[:,[0,1,2,3,4,5,6,8,9,10]]\n",
        "Y=df1[:,-4]\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   6.8594, 1007.9   ,   96.799 , ...,   10.605 ,    3.1547,\n",
              "          82.722 ],\n",
              "       [   6.785 , 1008.4   ,   97.118 , ...,   10.598 ,    3.2363,\n",
              "          82.776 ],\n",
              "       [   6.8977, 1008.8   ,   95.939 , ...,   10.601 ,    3.2012,\n",
              "          82.468 ],\n",
              "       ...,\n",
              "       [   7.2647, 1006.3   ,   99.496 , ...,   10.483 ,    7.9632,\n",
              "          90.912 ],\n",
              "       [   7.006 , 1006.8   ,   99.008 , ...,   10.533 ,    6.2494,\n",
              "          93.227 ],\n",
              "       [   6.9279, 1007.2   ,   97.533 , ...,   10.583 ,    4.9816,\n",
              "          92.498 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqqcddmwFkHF"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g1qhUIfFkCY"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.25,random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GDBJng8Fj9-"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkWZpFkDFj8O",
        "outputId": "bd427e29-d786-461e-e5a8-c2366672eb53"
      },
      "source": [
        "scaler=MinMaxScaler()\n",
        "scaler.fit(x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MinMaxScaler(copy=True, feature_range=(0, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LdpMVNkGC7H",
        "outputId": "03fd78ee-2e2d-4703-bec9-510d57eed0b7"
      },
      "source": [
        "x_train=scaler.transform(x_train)\n",
        "x_test=scaler.transform(x_test)\n",
        "x_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.35890393, 0.40602285, 0.91801706, ..., 0.34107329, 0.03084967,\n",
              "        0.48475958],\n",
              "       [0.55162803, 0.59086189, 0.72785444, ..., 0.42819611, 0.02833486,\n",
              "        0.43366477],\n",
              "       [0.69430373, 0.53478712, 0.55215014, ..., 0.14847583, 0.15186537,\n",
              "        0.33822331],\n",
              "       ...,\n",
              "       [0.29923532, 0.48494289, 0.94876603, ..., 0.77514199, 0.00101504,\n",
              "        0.41400706],\n",
              "       [0.64399376, 0.35825545, 0.50904718, ..., 0.04705791, 0.10100297,\n",
              "        0.36756316],\n",
              "       [0.3486443 , 0.24340602, 0.81637941, ..., 0.34416412, 0.00787964,\n",
              "        0.54170062]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhekVuokGC5l"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "# add nodes for prediction\n",
        "model.add(Dense(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2bEt6ZGGC2M"
      },
      "source": [
        "model.compile(optimizer='rmsprop',loss='mse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d6DG8b5GCxl",
        "outputId": "12763759-5711-4ff1-ad59-bd8ccafa6fc8"
      },
      "source": [
        "# Fit the model\n",
        "model.fit(x_train, y_train, epochs=250)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "353/353 [==============================] - 1s 1ms/step - loss: 15513.4893\n",
            "Epoch 2/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1844.4015\n",
            "Epoch 3/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 35.7032\n",
            "Epoch 4/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 23.4613\n",
            "Epoch 5/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 19.9265\n",
            "Epoch 6/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 17.2483\n",
            "Epoch 7/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 14.8818\n",
            "Epoch 8/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 12.9379\n",
            "Epoch 9/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 11.1952\n",
            "Epoch 10/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 9.6087\n",
            "Epoch 11/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 8.2239\n",
            "Epoch 12/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 6.9240\n",
            "Epoch 13/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 5.8025\n",
            "Epoch 14/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 4.7731\n",
            "Epoch 15/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 3.9035\n",
            "Epoch 16/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 3.1463\n",
            "Epoch 17/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 2.5466\n",
            "Epoch 18/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 2.0541\n",
            "Epoch 19/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.6902\n",
            "Epoch 20/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.4255\n",
            "Epoch 21/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.2437\n",
            "Epoch 22/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.1596\n",
            "Epoch 23/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.0857\n",
            "Epoch 24/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.0230\n",
            "Epoch 25/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.0027\n",
            "Epoch 26/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9731\n",
            "Epoch 27/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9519\n",
            "Epoch 28/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9588\n",
            "Epoch 29/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9334\n",
            "Epoch 30/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9291\n",
            "Epoch 31/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9284\n",
            "Epoch 32/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9267\n",
            "Epoch 33/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9298\n",
            "Epoch 34/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9268\n",
            "Epoch 35/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9355\n",
            "Epoch 36/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9270\n",
            "Epoch 37/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9332\n",
            "Epoch 38/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9223\n",
            "Epoch 39/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9215\n",
            "Epoch 40/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9210\n",
            "Epoch 41/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9252\n",
            "Epoch 42/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9036\n",
            "Epoch 43/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9183\n",
            "Epoch 44/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9132\n",
            "Epoch 45/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9165\n",
            "Epoch 46/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9233\n",
            "Epoch 47/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9200\n",
            "Epoch 48/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9242\n",
            "Epoch 49/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9045\n",
            "Epoch 50/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9150\n",
            "Epoch 51/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9162\n",
            "Epoch 52/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9058\n",
            "Epoch 53/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9075\n",
            "Epoch 54/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8923\n",
            "Epoch 55/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8917\n",
            "Epoch 56/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9132\n",
            "Epoch 57/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9044\n",
            "Epoch 58/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9011\n",
            "Epoch 59/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8889\n",
            "Epoch 60/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9031\n",
            "Epoch 61/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8956\n",
            "Epoch 62/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9032\n",
            "Epoch 63/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8981\n",
            "Epoch 64/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9040\n",
            "Epoch 65/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9030\n",
            "Epoch 66/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8961\n",
            "Epoch 67/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8961\n",
            "Epoch 68/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8939\n",
            "Epoch 69/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9008\n",
            "Epoch 70/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8906\n",
            "Epoch 71/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8804\n",
            "Epoch 72/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8811\n",
            "Epoch 73/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8925\n",
            "Epoch 74/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8805\n",
            "Epoch 75/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8873\n",
            "Epoch 76/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8778\n",
            "Epoch 77/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8788\n",
            "Epoch 78/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8844\n",
            "Epoch 79/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8790\n",
            "Epoch 80/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8805\n",
            "Epoch 81/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8868\n",
            "Epoch 82/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8685\n",
            "Epoch 83/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8804\n",
            "Epoch 84/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8798\n",
            "Epoch 85/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8935\n",
            "Epoch 86/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8617\n",
            "Epoch 87/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8820\n",
            "Epoch 88/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8700\n",
            "Epoch 89/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8731\n",
            "Epoch 90/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8697\n",
            "Epoch 91/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8715\n",
            "Epoch 92/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8635\n",
            "Epoch 93/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8752\n",
            "Epoch 94/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8682\n",
            "Epoch 95/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8698\n",
            "Epoch 96/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8598\n",
            "Epoch 97/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8754\n",
            "Epoch 98/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8638\n",
            "Epoch 99/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8727\n",
            "Epoch 100/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8590\n",
            "Epoch 101/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8634\n",
            "Epoch 102/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8585\n",
            "Epoch 103/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8665\n",
            "Epoch 104/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8530\n",
            "Epoch 105/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8560\n",
            "Epoch 106/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8726\n",
            "Epoch 107/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8647\n",
            "Epoch 108/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8560\n",
            "Epoch 109/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8628\n",
            "Epoch 110/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8640\n",
            "Epoch 111/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8463\n",
            "Epoch 112/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8470\n",
            "Epoch 113/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8552\n",
            "Epoch 114/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8539\n",
            "Epoch 115/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8551\n",
            "Epoch 116/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8399\n",
            "Epoch 117/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8415\n",
            "Epoch 118/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8420\n",
            "Epoch 119/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8457\n",
            "Epoch 120/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8348\n",
            "Epoch 121/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8566\n",
            "Epoch 122/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8539\n",
            "Epoch 123/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8653\n",
            "Epoch 124/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8562\n",
            "Epoch 125/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8397\n",
            "Epoch 126/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8579\n",
            "Epoch 127/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8488\n",
            "Epoch 128/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8487\n",
            "Epoch 129/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8458\n",
            "Epoch 130/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8406\n",
            "Epoch 131/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8569\n",
            "Epoch 132/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8315\n",
            "Epoch 133/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8374\n",
            "Epoch 134/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8476\n",
            "Epoch 135/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8419\n",
            "Epoch 136/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8572\n",
            "Epoch 137/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8387\n",
            "Epoch 138/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8206\n",
            "Epoch 139/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8619\n",
            "Epoch 140/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8263\n",
            "Epoch 141/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8408\n",
            "Epoch 142/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8356\n",
            "Epoch 143/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8283\n",
            "Epoch 144/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8439\n",
            "Epoch 145/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8402\n",
            "Epoch 146/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8316\n",
            "Epoch 147/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8359\n",
            "Epoch 148/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8288\n",
            "Epoch 149/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8161\n",
            "Epoch 150/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8366\n",
            "Epoch 151/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8249\n",
            "Epoch 152/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8254\n",
            "Epoch 153/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8241\n",
            "Epoch 154/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8229\n",
            "Epoch 155/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8340\n",
            "Epoch 156/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8295\n",
            "Epoch 157/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8171\n",
            "Epoch 158/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8450\n",
            "Epoch 159/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8246\n",
            "Epoch 160/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8266\n",
            "Epoch 161/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8190\n",
            "Epoch 162/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8208\n",
            "Epoch 163/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8286\n",
            "Epoch 164/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8180\n",
            "Epoch 165/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8259\n",
            "Epoch 166/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8163\n",
            "Epoch 167/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8191\n",
            "Epoch 168/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8204\n",
            "Epoch 169/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8236\n",
            "Epoch 170/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8150\n",
            "Epoch 171/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8097\n",
            "Epoch 172/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8132\n",
            "Epoch 173/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8180\n",
            "Epoch 174/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8196\n",
            "Epoch 175/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8100\n",
            "Epoch 176/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8249\n",
            "Epoch 177/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8180\n",
            "Epoch 178/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8115\n",
            "Epoch 179/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8151\n",
            "Epoch 180/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8134\n",
            "Epoch 181/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7986\n",
            "Epoch 182/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8095\n",
            "Epoch 183/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8163\n",
            "Epoch 184/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8173\n",
            "Epoch 185/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8159\n",
            "Epoch 186/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8168\n",
            "Epoch 187/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8006\n",
            "Epoch 188/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8168\n",
            "Epoch 189/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7981\n",
            "Epoch 190/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8086\n",
            "Epoch 191/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7924\n",
            "Epoch 192/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8082\n",
            "Epoch 193/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8180\n",
            "Epoch 194/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7933\n",
            "Epoch 195/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8070\n",
            "Epoch 196/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8064\n",
            "Epoch 197/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8057\n",
            "Epoch 198/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8246\n",
            "Epoch 199/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7961\n",
            "Epoch 200/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8128\n",
            "Epoch 201/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7951\n",
            "Epoch 202/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8123\n",
            "Epoch 203/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8014\n",
            "Epoch 204/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8055\n",
            "Epoch 205/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8055\n",
            "Epoch 206/250\n",
            "353/353 [==============================] - 1s 1ms/step - loss: 0.7950\n",
            "Epoch 207/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8009\n",
            "Epoch 208/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7991\n",
            "Epoch 209/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7959\n",
            "Epoch 210/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8036\n",
            "Epoch 211/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8121\n",
            "Epoch 212/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8111\n",
            "Epoch 213/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7992\n",
            "Epoch 214/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8009\n",
            "Epoch 215/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8003\n",
            "Epoch 216/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7962\n",
            "Epoch 217/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8112\n",
            "Epoch 218/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8022\n",
            "Epoch 219/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7900\n",
            "Epoch 220/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7986\n",
            "Epoch 221/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7837\n",
            "Epoch 222/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7954\n",
            "Epoch 223/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7879\n",
            "Epoch 224/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8008\n",
            "Epoch 225/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7893\n",
            "Epoch 226/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7935\n",
            "Epoch 227/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7971\n",
            "Epoch 228/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7850\n",
            "Epoch 229/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7921\n",
            "Epoch 230/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7896\n",
            "Epoch 231/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7952\n",
            "Epoch 232/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7995\n",
            "Epoch 233/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7923\n",
            "Epoch 234/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7974\n",
            "Epoch 235/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7952\n",
            "Epoch 236/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7915\n",
            "Epoch 237/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7821\n",
            "Epoch 238/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7975\n",
            "Epoch 239/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7825\n",
            "Epoch 240/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7940\n",
            "Epoch 241/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7782\n",
            "Epoch 242/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7925\n",
            "Epoch 243/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7898\n",
            "Epoch 244/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7934\n",
            "Epoch 245/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7859\n",
            "Epoch 246/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7825\n",
            "Epoch 247/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7883\n",
            "Epoch 248/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7912\n",
            "Epoch 249/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7967\n",
            "Epoch 250/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7922\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f247ccdc850>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "tMEPdFMCGgIU",
        "outputId": "c10e24d7-203b-451f-a46e-9b4f3d23e4cf"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib as plot\n",
        "model_loss = pd.DataFrame(model.history.history)\n",
        "model_loss.plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f247f6dcd90>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZiW-AxXGgE9"
      },
      "source": [
        "pred = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXMXNQmkGgD3",
        "outputId": "4f2162e4-a26d-4c22-d211-666c45a389f0"
      },
      "source": [
        "pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[133.16626 ],\n",
              "       [134.62343 ],\n",
              "       [111.64107 ],\n",
              "       ...,\n",
              "       [161.36105 ],\n",
              "       [105.017044],\n",
              "       [133.5266  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXtkP1SBGgCc"
      },
      "source": [
        "pred = pred.ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUJFrRhBGf-8",
        "outputId": "1a1af207-2404-4623-c64b-26e8683cc681"
      },
      "source": [
        "test_score = model.evaluate(x_test,y_test,verbose=0)\n",
        "test_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1187825202941895"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb1Sx1YFGf7u"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error,mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5npqyS6cGzlk",
        "outputId": "3dabe1b2-17db-428c-97a6-f642b3118b21"
      },
      "source": [
        "mean_absolute_error(pred,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.847077381864507"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5-zyZIDGzhG",
        "outputId": "9863b13b-e78b-43d0-830a-4e8803809f6b"
      },
      "source": [
        "mean_squared_error(pred,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1187823288252352"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aZ86y6YGzcy"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "Nxgoozs8GzXc",
        "outputId": "646d7e6d-142d-42dc-948f-cd2fdc2e6be9"
      },
      "source": [
        "plt.scatter(y_test,pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f247f0fd450>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}